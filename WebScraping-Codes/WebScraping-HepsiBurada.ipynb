{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbpXtimVc0bSVeSSqU/U3c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Giriş"],"metadata":{"id":"2b7kRrp-qWXh"}},{"cell_type":"markdown","source":["### Veritabanı Oluşturma"],"metadata":{"id":"ioO4pNXQ7ztz"}},{"cell_type":"markdown","source":["MongoDb de yapılan işlemler:\n","\n","1. Yeni bir Cluster oluşturuldu. İsmi: \"Cluster-WebScraping\"\n","2. Cluster-WebScraping için veritabanı erişim izni verilen iki kullanıcı oluşturuldu.\n","  \n","  * isim: zeyneperhan şifre: 20012022\n","  * isim: hazarkoc şifre:20012022 \n","\n","3. Cluster-WebScraping'e IP üzerinden otomatik bağlanmak için Ip adresi eklendi\n","\n","4. Cluster içerisine yeni veritabanı eklenip koleksiyon oluşturuldu."],"metadata":{"id":"m9m4-nCL5uwe"}},{"cell_type":"markdown","source":["### Pyhton ile Veritabanına Bağlanma"],"metadata":{"id":"BuAFvVgd8HB6"}},{"cell_type":"code","source":["# MongoDb işlemleri için gerekli kütüphaneler eklendi\n","import pymongo "],"metadata":{"id":"IsyqFcsB8ty1","executionInfo":{"status":"ok","timestamp":1666038155921,"user_tz":-180,"elapsed":9,"user":{"displayName":"Zeynep Aslı Erhan","userId":"03313672524286903446"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Cluster-WebScraping'e bağlanmak için :\n","\n","1. MongoDb Atlas veya MongoDb Compass'a göre seçim yapılır.\n","  \n","  * MongoDb Atlas'ı seçtiğimiz için MongoDB Drivers kısmı seçilir.\n","\n","2. Driver ve versiyonu seçilir.\n","  \n","  * Driver: Pyhton Versiyon: 3.4 ve sonrası\n","\n","  Burada verdiği linki direkt kopyalamamak gerekiyor!!! "],"metadata":{"id":"k9BfM5XU8vCX"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"fRIs4oX6vYtu","executionInfo":{"status":"ok","timestamp":1666038155923,"user_tz":-180,"elapsed":9,"user":{"displayName":"Zeynep Aslı Erhan","userId":"03313672524286903446"}}},"outputs":[],"source":["# Cluster-WebScraping için bağlantı sağlandı. (username: zeyneperhan password: 20012022)\n","myclient = pymongo.MongoClient(\"mongodb://zeynep:20012022@ac-akv12vk-shard-00-00.6erqfem.mongodb.net:27017,ac-akv12vk-shard-00-01.6erqfem.mongodb.net:27017,ac-akv12vk-shard-00-02.6erqfem.mongodb.net:27017/?ssl=true&replicaSet=atlas-8ffx15-shard-0&retryWrites=true&w=majority\") \n","\n","# Kullanacağımız veritabanı için erişim sağladık.\n","mydb = myclient[\"Bilgisayar\"]\n","\n","# Bilgisayar Veritabanındaki Amazon koleksiyonuna erişim sağladık\n","mycollectionAmazon= mydb[\"Amazon\"]"]},{"cell_type":"markdown","source":["# BeatifulSoup ile HepsiBurada Web Scraping"],"metadata":{"id":"Y_fZdUc0omFu"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4Zk1RD5-FyT","executionInfo":{"status":"ok","timestamp":1666038227059,"user_tz":-180,"elapsed":71143,"user":{"displayName":"Zeynep Aslı Erhan","userId":"03313672524286903446"}},"outputId":"39721cf1-4545-44b4-9c23-4e6f9b5daa8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------\n","Processing https://www.hepsiburada.com/laptop-notebook-dizustu-bilgisayarlar-c-98?sayfa=1...\n","-------------\n","<Response [200]>\n","-------------\n","Processing https://www.hepsiburada.com/laptop-notebook-dizustu-bilgisayarlar-c-98?sayfa=2...\n","-------------\n","<Response [200]>\n","-------------\n","Processing https://www.hepsiburada.com/laptop-notebook-dizustu-bilgisayarlar-c-98?sayfa=3...\n","-------------\n","<Response [200]>\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from time import sleep\n","\n","# Web Scraping yapılırken ilgili koleksiyona erişiliyor.\n","mycollectionHepsiBurada= mydb[\"HepsiBurada\"]\n","\n","# Web scraping ile veri aktarmadan önce koleksiyon içeriğini tamamen siliyoruz.\n","mycollectionHepsiBurada.delete_many({}) \n","\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0'}\n","\n","# Verileri çekeceğimiz sitenin ana url'si\n","base_url = 'https://www.hepsiburada.com/laptop-notebook-dizustu-bilgisayarlar-c-98?sayfa={0}'\n","\n","#veriyi tutan dict\n","item = {}\n","\n","for i in range(1, 4):\n","    \n","    print(\"-------------\")\n","    print('Processing {0}...'.format(base_url.format(i)))\n","    print(\"-------------\")    \n","\n","    # İstenilen sayfanın verileri alınıyor:\n","    response = requests.get(base_url.format(i), headers=headers)\n","    # Veri alımı başarılıysa response 200 olarak görülmeli 503 ise request işlemi hatalıdır\n","    print(response)\n","\n","    # BeautifulSoup ile verilerin içeriğini alıp html.parser ile parçalayıyor. soup içerisinde tutuyor\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Olduğumuz sayfadaki tüm ürünlerin bilgisi results içerisine aktarılır:\n","    results = soup.find_all('li',{'class': 'productListContent-zAP0Y5msy8OHn5z7T_K_'})\n","\n","\n","    # Results içerisindeki ürün kadar bu for döngüsü döner.\n","    for result in results:\n","\n","        product_name = result.h3.text\n","        item[\"product_name\"] = product_name\n","\n","        try:\n","            product_url = 'https://www.hepsiburada.com' + result.a.get(\"href\")\n","            item[\"product_url\"]= product_url\n","\n","            # Ürün hakkında detaylı bilgi edinmek için:\n","\n","            product_response = requests.get(product_url, headers=headers)\n","\n","            product_soup = BeautifulSoup(product_response.content, 'html.parser')\n","\n","            #------ Ürün Değerlendirilmesi-----\n","\n","\n","            if product_soup.find('span', {'class': 'rating-star'}) is not None:\n","                rating = product_soup.find('span', {'class': 'rating-star'}).text\n","            else:\n","              rating = None\n","\n","            item[\"rating\"]= rating\n","\n","\n","            #------ Fiyat-----\n","\n","            if product_soup.find(\"del\",{\"id\":\"originalPrice\"}) is not None:\n","              product_price = product_soup.find(\"del\",{\"id\":\"originalPrice\"}).text\n","            else:\n","              product_price = None\n","\n","            item[\"price\"]=product_price\n","\n","            product_details = product_soup.find_all(\"table\",{\"class\":\"data-list tech-spec\"})\n","\n","            for details in product_details:\n","\n","              informations = details.find_all(\"tr\")\n","\n","              for info in informations:\n","\n","                try:               \n","                    label = info.find(\"th\").text\n","                    value = info.find(\"span\").text\n","\n","                    item[label]=value\n","                    \n","                except AttributeError:\n","                  continue\n","\n","        except AttributeError:\n","            continue\n","\n","        # Bir tane veriyi mongoDB'ye ekler\n","        mycollectionHepsiBurada.insert_one(item)\n","\n","        # MongoDB'ye birer tane veri aktarmak istediğimiz için item içerisi silinir\n","        item.clear()"]},{"cell_type":"markdown","source":["## Not:"],"metadata":{"id":"CnXM_JIW2SGn"}},{"cell_type":"markdown","source":["\n","❗ Çektiğiniz bilgileri \"print(xxx)\" ile ekrana bastırırsanız ne üzerinde çalıştığınızı anlamanız kolaylaşır.\n","---\n","***ÖRNEKLER:***\n","\n","1. Çalıştığınız url'den geri dönüş değerinin 200 veya 503 olduğunu inceleyerek başarılı veya başarısız istek olduğunu anlayabilirsiniz:\n","```\n","product_response = requests.get(product_url, headers=headers)\n","print(product_response)\n","```\n","\n","2. \".prettify()\" kodu çektiğiniz html bilgisini print ederken daha düzenli hale getirir. Aşağıdaki response içeriğindeki html bilgilerini \"print(soap)\" diye de inceleyebilirsiniz.\n","```\n","response = requests.get(base_url + '&page={0}'.format(i), headers=headers)\n","soup = BeautifulSoup(response.content, 'html.parser')\n","print(soup.prettify())\n","```\n","3. html içeriğinde bulmak istediğiniz kısmın doğru olup olmadığını anlamak adına da değişkenleri ekrana bastırabilirsiniz.\n","```\n","results = soup.find_all('div',{'class': 'a-section a-spacing-base'})\n","print(results)\n","```\n","<br/>\n","\n","❗ Bu işlemler web scraping konusunda hatta derin öğrenme gibi konularda bile hangi veri üzerinde işlemler yaptığınızı anlamak adına kolaylık sağlıyor. \n","\n","\n","\n","\n","\n"],"metadata":{"id":"PbEvxFpuF3eQ"}}]}